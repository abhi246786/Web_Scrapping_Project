{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements\n",
    "#pip3 install requests\n",
    "#pip3 install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic fundamentals of web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is with html tags : <title>Easy Python â€“ A programming language of revolution</title>\n",
      "this is without html tags: Easy Python\n",
      "<a class=\"screen-reader-text skip-link\" href=\"#content\">Skip to content</a>\n"
     ]
    }
   ],
   "source": [
    "# import these two modules bs4 for selecting HTML tags easily\n",
    "from bs4 import BeautifulSoup\n",
    "# requests module is easy to operate some people use urllib but I prefer this one because it is easy to use.\n",
    "import requests\n",
    "\n",
    "# I put here my own blog url ,you can change it.\n",
    "url=\"https://getpython.wordpress.com/\"\n",
    "\n",
    "#Requests module use to data from given url\n",
    "source=requests.get(url)\n",
    "\n",
    "# BeautifulSoup is used for getting HTML structure from requests response.(craete your soup)\n",
    "soup=BeautifulSoup(source.text,'html')\n",
    "\n",
    "# Find function is used to find a single element if there are more than once it always returns the first element.\n",
    "title=soup.find('title') # place your html tagg in parentheses that you want to find from html.\n",
    "print(\"this is with html tags :\",title)\n",
    "\n",
    "qwery=soup.find('h1') # here i find first h1 tagg in my website using find operation.\n",
    "\n",
    "#use .text for extract only text without any html tags\n",
    "print(\"this is without html tags:\",qwery.text) \n",
    "\n",
    "\n",
    "links=soup.find('a') #i extarcted link using \"a\" tag\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extarct data from innerhtml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#content\n"
     ]
    }
   ],
   "source": [
    "# here i extarcted href data from anchor tag.\n",
    "print(links['href']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['screen-reader-text', 'skip-link']\n"
     ]
    }
   ],
   "source": [
    "# similarly i got class details from a anchor tag\n",
    "print(links['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## findall operation in Bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findall function is used to fetch all tags at a single time.\n",
    "many_link=soup.find_all('a') # here i extracted all the anchor tags of my website\n",
    "total_links=len(many_link) # len function is use to calculate length of your array\n",
    "print(\"total links in my website :\",total_links)\n",
    "print()\n",
    "for i in many_link[:6]: # here i use slicing to fetch only first 6 links from rest of them.\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_link=many_link[1] #here i fetch second link which place on 1 index number in many_links.\n",
    "print(second_link)\n",
    "print()\n",
    "print(\"href is :\",second_link['href']) #only href link is extracted from ancor tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select div tag from second link\n",
    "nested_div=second_link.find('div')\n",
    "# As you can see div element extarcted , it also have inner elements\n",
    "print(nested_div)\n",
    "print()\n",
    "#here i extracted class element from div but it give us in the form of list\n",
    "z=(nested_div['class'])\n",
    "print(z)\n",
    "print(type(z))\n",
    "print()\n",
    "#  \" \" .join () method use to convert list type  into string type\n",
    "print(\"class name of div is :\",\" \".join(nested_div['class'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrap data from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki=requests.get(\"https://en.wikipedia.org/wiki/World_War_II\")\n",
    "soup=BeautifulSoup(wiki.text,'html')\n",
    "print(soup.find('title'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find html tags with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww2_contents=soup.find_all(\"div\",class_='toc')\n",
    "for i in ww2_contents:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview=soup.find_all('table',class_='infobox vevent')\n",
    "for z in overview:\n",
    "    print(z.text)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
